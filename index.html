<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shraman Pramanick</title>
  
  <meta name="author" content="Shraman Pramanick">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/png" href="images/Blue_Jays.png">
  <title>Shraman Pramanick</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-113195086-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-113195086-1');
</script>

<style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #2d59eb;
      text-decoration: none;
    }
    b {
      color: #000000;
      text-decoration: none;
    }
    c {
      color: #0BDA51;
      text-decoration: none;
    }
    d {
      color: #FFA500;
      text-decoration: none;
    }
    a:focus,
    a:hover {
      color: #af19bf;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px
      background-image: "images/jose/bg.png";
       /*background-color: #fffff;*/
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 24px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14.5px;
      font-weight: 600
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 34px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      IIIIIItransition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }I
    
    span.highlight {
      Ibackground-color: #CA7FD4;
    }
  </style>

  <style>
    .custom-link {
        color: black; /* Set the link color to black */
        text-decoration: none; /* Remove underline by default */
    }
    .custom-link:hover {
        text-decoration: underline; /* Add underline on hover */
    }

    img {
        /* Syntax: box-shadow: horizontal-offset vertical-offset blur-radius spread-radius color; */
        box-shadow: 0.5px 0.5px 1.5px 0.5px rgba(0, 0, 0, 1);
        /* Adjust the values and color as needed */
    }
  </style>

</head>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
               <name>Shraman Pramanick</name>
              </p>
              <p style="text-align:justify">
            Hi, I am Shraman (pronounced as 'shra'--'man'). I received my Ph.D. at <a href="https://www.jhu.edu/">Johns Hopkins University</a>, advised by Bloomberg Distinguished Professor <a href="https://engineering.jhu.edu/faculty/rama-chellappa/">Rama Chellappa</a>. My research focuses on applications of deep learning and computer vision to multimodal learning (vision + X), egocentric vision and video-language pre-training.
              </p>
              <p style="text-align:justify">
	            I have been fortunate to work with amazing minds from academia and industry during my Ph.D. In the summer and fall of 2024, I interned at FAIR Perception, Meta with <a href="https://www.robots.ox.ac.uk/~afourast/">Daffy</a> on fine-grained video temporal grounding. Previously, during the fall of 2023 and spring of 2024, I worked as a student researcher at <a href="https://research.google/"> Google Research </a> with <a href="https://vsubhashini.github.io/">Subhashini</a> on AI for Science. In the summer of 2023, I collaborated with <a href="https://www.linkedin.com/in/nicolas-ballas-a188583/">Nicolas</a>, <a href="https://www.linkedin.com/in/amjadalmahairi/">Amjad</a>, <a href="https://guangxinghan.github.io/">Guangxing</a>, <a href="https://wqfcr.github.io/">Qifan</a> on multimodal large language models. I also contributed to the <a href="http://ego-exo4d-data.org/">Ego-Exo4D</a> project to develop strong baselines for different downstream applications. During the summer of 2022, I worked with <a href="https://pzzhang.github.io/pzzhang/">Pengchuan</a> and <a href="http://people.csail.mit.edu/yalesong/home/">Yale</a> on egocentric video-language pre-training. I also collaborated with <a href="https://en.wikipedia.org/wiki/Yann_LeCun">Yann LeCun</a> and <a href="http://jingli.io/">Li Jing</a> on dimension-contrastive image-text pre-training.
             </p>  
              <p style="text-align:justify">
              Before joining Hopkins, I worked as a research assistant in a series of collaborative projects between <a href="https://www.qf.org.qa/research/qatar-computing-research-institute">QCRI</a> and <a href="https://www.iiitd.ac.in/">IIIT-Delhi</a>. During my undergrad days, I interned at the <a href="https://www.umontreal.ca/en/">University of Montreal</a> with <a href="https://www.mitacs.ca/en/programs/globalink/globalink-research-internship">Mitacs Globalink Fellowship</a> in the summer of 2019. I graduated from <a href="http://www.jaduniv.edu.in/">Jadavpur University, India</a>, in 2020 with my Bachelor's degree majoring in Electronics and Telecommunication Engineering.
              </p>

              <p style="text-align:center">
                <a href="mailto:spraman3@jhu.edu">Email</a> &nbsp/&nbsp
                <a href="data/Resume_Shraman_March_2025.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=20SubC8AAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ShramanPramanick">Github</a> &nbsp/&nbsp
		<a href="https://x.com/shramanpramani2">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/shramanpramanick/">LinkedIn</a> 
              </p>
            </td>
            <td style="padding:0.75%;width:28%;max-width:28%">
              <a href="images/JonBarron.jpg"><img style="width:90%;max-width:90%; box-shadow: 0px 0px 0px 0px rgba(0, 0, 0, 0)" alt="profile photo" src="images/Shraman_Photo_2023_Cropped.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-10px"><tbody>
            <tr style="padding:0px">
            <td style="padding:2.5%;vertical-align:middle;width:63%">
      <a id="news"><heading>News</heading></a>
      <p>
            <div style="width:100%;overflow-y:scroll; height:200px;">
                <ul id="News">
        <p>
        </p>
	<li> [June, 2025] <a href="">ED-VTG</a> is accepted in <b>ICCV 2025</b> as a Highlights.
	</li>
	<li> [December, 2024] Invited talk on <a href="https://arxiv.org/pdf/2407.09413">SPIQA</a> at Voxel51 NeurIPS 2024 Preshow. Recording can be found <a href="https://www.youtube.com/watch?v=p56kAbdYtUg">here</a>.
	</li>
	<li> [October, 2024] Invited lecture on <span style="color: brown;">Introduction to Transformers</span> in EN.520.665 Machine Perception at Johns Hopkins. Recording can be found <a href="https://drive.google.com/file/d/1BbU8qiFnJbCQDgdpsFuJvVajahYSDPr-/view?usp=sharing">here</a>.
	</li>
	<li> [September, 2024] <a href="https://arxiv.org/pdf/2407.09413">SPIQA</a> is accepted in <b>NeurIPS 2024 D&B</b>.
	</li>
	<li> [July, 2024] Joined <b>FAIR, Meta</b> as a returning research scientist intern. Working with <a href="https://www.robots.ox.ac.uk/~afourast/">Daffy</a>, <a href="https://www.linkedin.com/in/effrosyni-mavroudi-144a7a85/">Efi</a>, <a href="https://people.csail.mit.edu/yalesong/home/">Yale</a> and <a href="https://ai.meta.com/people/423618666731395/lorenzo-torresani/">Lorenzo</a> on fine-grained video temporal grounding. 
	</li>
	<li> [June, 2024] <a href="https://arxiv.org/pdf/2307.05463.pdf">EgoVLPv2</a> is awarded as an <a href="https://egovis.github.io/awards/2022_2023/">EgoVis (Egocentric Vision) 2022/2023 Distinguished Paper</a>.
	</li>
	<li> [June, 2024] Received <a href="https://drive.google.com/file/d/1pI-XO8W0Qlnbg-QT7aDsn6gqiFx7dlgH/view?usp=sharing">Spot Bonus</a> from Google for exceptional contributions while being a student researcher.
	</li>
	<li> [April, 2024] <a href="https://arxiv.org/pdf/2307.05463.pdf">VistaLLM</a> is selected as a Highlight (Top 2.8%) and <a href="https://arxiv.org/pdf/2307.05463.pdf">Ego-Exo4D</a> is selected as an Oral (Top 0.8%) in <b>CVPR 2024</b>.
	</li>
	<li > [March, 2024] Selected to participate in <b>Doctoral Consortium</b> at <b>CVPR 2024</b>. </span>
        </li>
	<li > [February, 2024] <a href="https://arxiv.org/pdf/2307.05463.pdf">VistaLLM</a> and <a href="https://arxiv.org/pdf/2307.05463.pdf">Ego-Exo4D</a> are accepted in <b>CVPR 2024</b>. </span>
        </li>
	<li> [February, 2024] Invited talk at <a href="https://ccvl.jhu.edu/">CCVL</a> on <a href="https://arxiv.org/pdf/2307.05463.pdf">EgoVLPv2</a> and <a href="https://arxiv.org/pdf/2307.05463.pdf">VistaLLM</a>. Recording can be found <a href="https://www.cis.jhu.edu/~shraman/Presentation_Recordings/AlanLab_Talk_Shraman_16Feb2024.mp4">here</a>.
        </li>
	<li> [December, 2023] Successfully passed the Graduate Board Oral (GBO) examination. Officially a Ph.D. candidate now. Thanks to my committee members <a href="https://engineering.jhu.edu/faculty/rama-chellappa/">Rama</a>, <a href="https://cogsci.jhu.edu/directory/alan-yuille/">Alan</a>, <a href="https://engineering.jhu.edu/faculty/vishal-patel/">Vishal</a>, <a href="https://www.cs.umd.edu/people/abhinav">Abhinav</a> and <a href="https://engineering.jhu.edu/faculty/anqi-angie-liu/">Anqi</a>.
        </li>
	<li> [October, 2023] Joined <b>Google Research</b> as a student researcher. Working with <a href="https://vsubhashini.github.io/">Subhashini</a> on AI for Science.
        </li>
	<li> [August, 2023] <a href="https://arxiv.org/pdf/2210.04135.pdf">VoLTA</a> is accepted in <b>TMLR 2023</b>.
	</li>
	<li> [July, 2023] <a href="https://arxiv.org/pdf/2307.05463.pdf">EgoVLPv2</a> and <a href="https://arxiv.org/pdf/2307.16715.pdf">UniVTG</a> are accepted in <b>ICCV 2023</b>.
	</li>
	<li> [June, 2023] Received student researcher offer from Google Research for Fall 2023.
	</li>
        <li> [June, 2023] Joined <b>Meta AI</b> as a returning research scientist intern. Working with <a href="https://www.linkedin.com/in/nicolas-ballas-a188583/">Nicolas</a>, <a href="https://www.linkedin.com/in/amjadalmahairi/">Amjad</a>, <a href="https://guangxinghan.github.io/">Guangxing</a>, <a href="https://wqfcr.github.io/">Qifan</a> and <a href="https://www.linkedin.com/in/rayhou/">Rui</a>.
        </li>
        <li> [February, 2023] 1 paper accepted in <b>Biomedical Signal Processing and Control</b>, Elsevier.
        </li>
      <li> [January, 2023] Received research internship offers from Meta AI (return), Microsoft Research, Adobe Research, Salesforce Research, and Samsung Research America for Summer 2023.
        </li>
      <li> [October, 2022] Attended <b>ECCV 2022</b>, and presented our work <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2204.13861&sa=D&sntz=1&usg=AOvVaw1BYho5NwCrYK2z_9DG9hMj">Where in the World is this Image? Transformer-based Geo-localization in the Wild</a>. 
        </li>
        </li>
      <li> [July, 2022] 1 paper accepted in <b>ECCV 2022</b>. 
        </li>
      <li> [May, 2022] Joined <b>Meta AI</b> as a research scientist intern. Working with <a href="https://pzzhang.github.io/pzzhang/">Pengchuan</a>, <a href="http://jingli.io/">Li</a>, <a href="http://people.csail.mit.edu/yalesong/home/">Yale</a>, and <a href="http://yann.lecun.com/">Yann</a>.
        </li>
      <li> [January, 2022] Attended <b>WACV 2022</b> in person at Waikoloa, Hawaii, and presented our work <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2110.10949&sa=D&sntz=1&usg=AOvVaw1VF3ivOTXAAwz9ZuU8LMu7">Multimodal Learning using Optimal Transport for Sarcasm and Humor Detection</a>.
        </li>
      <li> [November, 2021] Received research internship offers from Meta AI, AWS AI, and Adobe Research for Summer 2022.
        </li>
      <li> [October, 2021] Received EMNLP volunteer fellowship and free registration in EMNLP 2021.
        </li>
      <Ii> [October, 2021] 1 paper accepted in <b>WACV 2022</b>.
        </li>
      <li> [August, 2021] 1 paper accepted in <b>Findings of EMNLP 2021</b>.
        </li>
      <li> [July, 2021] Received ACL volunteer fellowship and free registration in ACL 2021.
        </li>
      <li> [May, 2021] 1 paper accepted in <b>Knowledge-based Systems</b>, Elsevier.
        </li>
      <li> [May, 2021] 1 paper accepted in <b>Findings of ACL 2021</b>. 
        </li>
        <li> [March, 2021] 1 paper accepted in <b>ICWSM 2021</b>. 
        </li>
        <li> [January, 2021] Joined <b>Johns Hopkins University</b> for my Ph.D. with <b>ECE departmental fellowship</b>, under the supervision of Professor <a href="https://engineering.jhu.edu/faculty/rama-chellappa/">Rama Chellappa</a>.
        </li>
        <li> [June, 2020] Joined <b>Qatar Computing Research Institute</b> (QCRI), Doha as a Research Associate.
        </li>
        <li> [April, 2020] Received Ph.D. admission offers from 7 US Universities - JHU, GaTech, Purdue, UCR, Penn State, WashU St. Louis, and Yale. 
        </li>
        <li> [January 2019] I have been selected as one of the <a href="https://www.mitacs.ca/en/programs/globalink/globalink-research-internship">Mitacs Globalink Research Interns</a> 2019.
        </li>
        
            </ul>
            </div>
      </p>
    </td>
          </tr>
        </tbody></table>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a id="news"><heading>Research</heading></a>
              <p> Representative papers are <span style="background-color: #ffffe6">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


	<tr bgcolor="#ffffe6">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/ED-VTG_Overview.png" alt="SPIQA" width="180">
            </div>
            </td>
            <td width="75%" valign="middle">
                <a href="" class="custom-link"><papertitle>Enrich and Detect: Video Temporal Grounding with Multimodal LLMs</papertitle></a>
            <br>
            <b>Shraman Pramanick</b>, <a href="https://scholar.google.com/citations?user=vYRzGGEAAAAJ&hl=en" class="custom-link">Effrosyni Mavroudi</a>, <a href="https://people.csail.mit.edu/yalesong/home/" class="custom-link">Yale Song</a>, <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/" class="custom-link">Rama Chellappa</a>, <a href="https://ltorresa.github.io/home.html" class="custom-link">Lorenzo Torresani</a>, <a href="https://www.robots.ox.ac.uk/~afourast/" class="custom-link">Triantafyllos Afouras</a>
            <br>
            <em><span style="color: brown;">ICCV 2025</span> </em>
            <br>
            </td>
          </tr>
          <td><br></td>


          <tr bgcolor="#ffffe6">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/SPIQA_Tasks_2.png" alt="SPIQA" width="180" height="76.5">
            </div>
            </td>
            <td width="75%" valign="middle">
                <a href="https://github.com/google/spiqa" class="custom-link"><papertitle>SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers</papertitle></a>
            <br>
            <b>Shraman Pramanick*</b>, <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/" class="custom-link">Rama Chellappa</a>, <a href="https://vsubhashini.github.io/" class="custom-link">Subhashini Venugopalan*</a>
            <br>
            <em><span style="color: brown;">NeurIPS 2024 D&B</span> </em>
            <br>
            [<a href="https://arxiv.org/pdf/2407.09413">Paper</a>] [<a href="https://github.com/google/spiqa">Code</a>] [<a href="https://huggingface.co/datasets/google/spiqa">Dataset</a>] [<a href="https://drive.google.com/file/d/1rt_isMA95EMmtlsqzbAnSfdbok8XRbbh/view?usp=drive_link">Poster</a>]
            </td>
          </tr>
          <td><br></td>

          <tr>
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/VistaLLM_Sampling.png" alt="VistaLLM_Sampling" width="180" height="80">
            </div>
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2312.12423" class="custom-link"> <papertitle>Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model</papertitle> </a>         
            <br>
            <b>Shraman Pramanick*</b>, <a href="https://guangxinghan.github.io/" class="custom-link">Guangxing Han*</a>, <a href="https://scholar.google.com/citations?hl=en&user=PKHKqX0AAAAJ" class="custom-link">Rui Hou</a>, <a href="https://sayannag.github.io/" class="custom-link">Sayan Nag</a>, <a href="https://sites.google.com/site/sernam" class="custom-link">Ser-Nam Lim</a>, <a href="https://scholar.google.ca/citations?user=euUV4iUAAAAJ&hl=en" class="custom-link">Nicolas Ballas</a>, <a href="https://wqfcr.github.io/" class="custom-link">Qifan Wang</a>, <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/" class="custom-link">Rama Chellappa</a>, <a href="https://scholar.google.com/citations?user=WbYAa7IAAAAJ" class="custom-link">Amjad Almahairi</a>    
            <br>
            <em><span style="color: brown;">CVPR 2024 (Highlight, Top 2.8%)</span> </em>
            <br>
            [<a href="https://arxiv.org/abs/2312.12423">Paper</a>] [<a href="https://shramanpramanick.github.io/VistaLLM/">Project</a>] [Code] [<a href="https://drive.google.com/file/d/1Hfnru-HPuV43ebAS2o19SfD40JrjMZOP/view?usp=drive_link">Poster</a>]
            </td>
          </tr>
          <td><br></td>


          <tr>
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/Ego-Exo4D.png" alt="EgoVLPv2" width="180" height="120">
            </div>
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2311.18259.pdf" class="custom-link"> <papertitle>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives</papertitle> </a>          
            <br>
            <a href="https://www.cs.utexas.edu/users/grauman/" class="custom-link">Kristen Grauman </a> et al.
            <br>
            <em><span style="color: brown;">CVPR 2024 (Oral, Top 0.8%)</span> </em>
            <br>
            [<a href="https://arxiv.org/pdf/2311.18259.pdf">Paper</a>] [<a href="https://ego-exo4d-data.org/">Project</a>] [<a href="https://ai.meta.com/blog/ego-exo4d-video-learning-perception/">Blog</a>] [<a href="https://youtu.be/GdooXEBAnI8">Video</a>]
            </td>
          </tr>
          <td><br></td>


          <tr bgcolor="#ffffe6">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <divI style="text-align: center">
              <img src="images/radar_egovlpv2-transparent.png" alt="EgoVLPv2" width="180" height="165">
            </div>
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2307.05463.pdf" class="custom-link"> <papertitle>EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone</papertitle></a>           
            <br>
            <b>Shraman Pramanick</b>, <a href="http://people.csail.mit.edu/yalesong/home/" class="custom-link">Yale Song</a>, <a href="https://sayannag.github.io/" class="custom-link">Sayan Nag</a>, <a href="https://qinghonglin.github.io/" class="custom-link">Kevin Qinghong Lin</a>, <a href="https://www.linkedin.com/in/hardik-shah-75ab5429/" class="custom-link">Hardik Shah</a>, <a href="https://sites.google.com/view/showlab" class="custom-link">Mike Z. Shou</a>, <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/" class="custom-link">Rama Chellappa</a>, <a href="https://qinghonglin.github.io/" class="custom-link">Pengchuan Zhang</a>    
            <br>
            <em><span style="color: brown;">ICCV 2023</span></em>
            <br>
            [<a href="https://arxiv.org/pdf/2307.05463.pdf">Paper</a>] [<a href="https://github.com/facebookresearch/EgoVLPv2">Code</a>] [<a href="https://shramanpramanick.github.io/EgoVLPv2/">Project</a>] [<a href="https://drive.google.com/file/d/1RcnF4paWFDWCTKn9v4nM5pXd31FxdXmD/view?usp=drive_link">Poster</a>]
            </td>
          </tr>
          <td><br></td>


          <tr>
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/UniVTG.jpg" alt="UniVTG" width="180" height="100" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2307.16715.pdf" class="custom-link"> <papertitle>UniVTG: Towards Unified Video-Language Temporal Grounding</papertitle></a>           
            <br>
            <a href="https://qinghonglin.github.io/" class="custom-link">Kevin Qinghong Lin</a>, <a href="https://qinghonglin.github.io/" class="custom-link">Pengchuan Zhang</a>, Joya Chen, <b>Shraman Pramanick</b>, Difei Gao, Alex JP. Wang, Rui Yan, <a href="https://sites.google.com/view/showlab" class="custom-link">Mike Z. Shou</a>   
            <br>
            <em><span style="color: brown;">ICCV 2023</span></em>
            <br>
            [<a href="https://arxiv.org/pdf/2307.16715.pdf">Paper</a>] [<a href="https://github.com/showlab/UniVTG">Code</a>]
            </td>
          </tr>
          <td><br></td>


          <tr bgcolor="#ffffe6">
            <td style="padding:15px;width:25%;vertical-align:middle;padding-right:20px">
              <div style="text-align: center">
              <img src="images/VoLTA_Alignment.png" alt="VoLTA" width="180" height="135" class="center">
            </div>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2210.04135.pdf" class="custom-link"> <papertitle>VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment</papertitle></a>           
            <br>
            <b>Shraman Pramanick*</b>, <a href="http://jingli.io/" class="custom-link">Li Jing*</a>, <a href="https://sayannag.github.io/" class="custom-link">Sayan Nag*</a>, <a href="https://jiachenzhu.github.io/" class="custom-link">Jiachen Zhu</a>, <a href="https://www.linkedin.com/in/hardik-shah-75ab5429/" class="custom-link">Hardik Shah</a>, <a href="https://en.wikipedia.org/wiki/Yann_LeCun" class="custom-link">Yann LeCun</a>, <a href="https://engineering.jhu.edu/faculty/rama-chellappa/" class="custom-link">Rama Chellappa</a>
            <br>
            <em><span style="color: brown;">TMLR 2023</span></em>
            <br>
            [<a href="https://arxiv.org/pdf/2210.04135.pdf">Paper</a>] [<a href="https://github.com/ShramanPramanick/VoLTA">Code</a>] [<a href="https://shramanpramanick.github.io/VoLTA/">Project</a>] </p>
            </td>
          </tr>
          <td><br></td>

          <tr>
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center;">
              <img src="images/TransLocator_System.png" alt="TransLocator" width="180" height="190" class="center">
            </div>
            </td>
            <td width="75%" valign="middle">
                 <a href="https://arxiv.org/pdf/2204.13861.pdf" class="custom-link"> <papertitle>Where in the World is this Image? Transformer-based Geo-localization in the Wild</papertitle></a>
            <br>
            <b>Shraman Pramanick</b>, <a href="https://ewanowara.github.io/" class="custom-link">Ewa Nowara</a>, Joshua Gleason, <a href="https://www.linkedin.com/in/carlos-castillo-7800113a/" class="custom-link">Carlos D. Castillo</a>, <a href="https://engineering.jhu.edu/faculty/rama-chellappa/" class="custom-link">Rama Chellappa</a>
            <br>
            <em><span style="color: brown;">ECCV 2022</span></em>
            <br>
            [<a href="https://arxiv.org/pdf/2204.13861.pdf">Paper</a>] [<a href="https://github.com/ShramanPramanick/Transformer_Based_Geo-localization">Code + Data</a>] [<a href="https://www.cis.jhu.edu/~shraman/TransLocator/ECCV_2022_TransLocator_Slides.pdf">Slides</a>] [<a href="https://www.cis.jhu.edu/~shraman/TransLocator/TransLocator_Poster.pdf">Poster</a>] [<a href="https://www.cis.jhu.edu/~shraman/TransLocator/TransLocator_Presentation.mp4">Video</a>] </p>
            </td>
          </tr>
          <td><br></td>

          <tr>
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center;">
              <img src="images/MuLOT_System.png" alt="MuLOT" width="180" height="125" class="center">
            </div>
            </td>
            <td width="75%" valign="middle">
                 <a href="https://arxiv.org/pdf/2110.10949.pdf" class="custom-link"> <papertitle>Multimodal Learning using Optimal Transport for Sarcasm and Humor Detection</papertitle></a>           
            <br>
            <b>Shraman Pramanick*</b>, <a href="https://sites.google.com/site/aniketsealiitkgp/home" class="custom-link">Aniket Roy*</a>, <a href="https://engineering.jhu.edu/vpatel36/team/vishalpatel/" class="custom-link">Vishal M. Patel</a>
            <br>
            <em><span style="color: brown;">WACV 2022</span></em>
            <br>
            [<a href="https://arxiv.org/pdf/2110.10949.pdf">Paper</a>] [<a href="https://www.cis.jhu.edu/~shraman/MuLOT/WACV_2022_MuLOT_slides.pdf">Slides</a>] [<a href="https://www.cis.jhu.edu/~shraman/MuLOT/MuLOT_Poster.pdf">Poster</a>] [<a href="https://www.cis.jhu.edu/~shraman/MuLOT/MuLOT_Presentation.mp4">Video</a>] </p>
            </td>
          </tr>
          <td><br></td>

          <tr>
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center;">
              <img src="images/MOMENTA_System.png" alt="MuLOT" width="180" height="70" class="center">
            </div>
            </td>
            <td width="75%" valign="middle">
                 <a href="https://arxiv.org/pdf/2109.05184.pdf" class="custom-link"> <papertitle>MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets</papertitle></a>           
            <br>
            <b>Shraman Pramanick*</b>, <a href="https://scholar.google.com/citations?user=RzntnxoAAAAJ&hl=en" class="custom-link">Shivam Sharma*</a>, <a href="https://scholar.google.com/citations?user=CJcR_T8AAAAJ&hl=en" class="custom-link">Dimitar Dimitrov</a>, <a href="https://www.iiitd.ac.in/shad" class="custom-link">Shad Aktar</a>, <a href="https://mbzuai.ac.ae/study/faculty/preslav-nakov" class="custom-link">Preslav Nakov</a>, <a href="https://tanmoychak.com/" class="custom-link">Tanmoy Chakraborty</a> 
            <br>
            <em><span style="color: brown;">Findings of EMNLP 2021</span></em>
            <br>
            [<a href="https://arxiv.org/pdf/2109.05184.pdf">Paper</a>] [<a href="https://github.com/lcs2-iiitd/momenta">Code + Data</a>] [<a href="https://www.cis.jhu.edu/~shraman/MOMENTA_poster.pdf">Poster</a>] [<a href="https://www.cis.jhu.edu/~shraman/MOMENTA_slides.pdf">Slides</a>] </p>
            </td>
          </tr>
          <td><br></td>

          <tr>
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center;">
              <img src="images/BCI_System_Overview.png" alt="BCI" width="180" height="110" class="center">
            </div>
            </td>
            <td width="75%" valign="middle">
                 <a href="https://www.sciencedirect.com/science/article/pii/S1746809423001982" class="custom-link"> <papertitle>Localizing and Grasping of 3-D Objects by a Vision-Actuated Robot Arm using Brain-Computer Interface</papertitle></a>           
            <br>
            <a href="https://scholar.google.com/citations?hl=en&user=KFmLY9EAAAAJ&view_op=list_works&sortby=pubdate" class="custom-link">Arnab Rakshit</a>, <b>Shraman Pramanick</b>, <a href="https://scholar.google.com/citations?user=H6ULoLQAAAAJ&hl=en" class="custom-link">Anurag Bagchi</a>, <a href="https://scholar.google.co.in/citations?user=-k1O0oEAAAAJ&hl=en" class="custom-link">Saugat Bhattacharyya</a>
            <br>
            <em><span style="color: brown;">Biomedical Signal Processing and Control, Elsevier, 2023</span></em>
            <br>
            [<a href="https://www.sciencedirect.com/science/article/pii/S1746809423001982">Paper</a>] </p>
            </td>
          </tr>
          <td><br></td>

          <tr>
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center;">
              <img src="images/ACL_Harmful_Memes_Explainability.png" alt="ACL_Harmful_Memes" width="180" height="150" class="center">
            </div>
            </td>
            <td width="75%" valign="middle">
                 <a href="https://aclanthology.org/2021.findings-acl.246.pdf" class="custom-link"> <papertitle>Detecting Harmful Memes and Their Targets </papertitle></a>           
            <br>
            <b>Shraman Pramanick</b>, <a href="https://scholar.google.com/citations?user=CJcR_T8AAAAJ&hl=en" class="custom-link">Dimitar Dimitrov</a>, <a href="https://scholar.google.com/citations?user=PwEJRbwAAAAJ&hl=en" class="custom-link">Rituparna Mukherjee</a>, <a href="https://scholar.google.com/citations?user=RzntnxoAAAAJ&hl=en" class="custom-link">Shivam Sharma</a>, <a href="https://www.iiitd.ac.in/shad" class="custom-link">Shad Aktar</a>, <a href="https://mbzuai.ac.ae/study/faculty/preslav-nakov" class="custom-link">Preslav Nakov</a>, <a href="https://tanmoychak.com/" class="custom-link">Tanmoy Chakraborty</a> 
            <br>
            <em><span style="color: brown;">Findings of ACL 2021</span></em>
            <br>
            [<a href="https://aclanthology.org/2021.findings-acl.246.pdf">Paper</a>] [<a href="https://github.com/di-dimitrov/harmeme">Code + Data</a>] [<a href="https://www.cis.jhu.edu/~shraman/ACL_2021_Harmeme_Presentation.pdf">Slides</a>] [<a href="https://www.youtube.com/watch?v=E7TswYcUqd8&t=58s">Video</a>] </p>
            </td>
          </tr>
          <td><br></td>

          <tr>
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center;">
              <img src="images/FLORAL_System.png" alt="FLORAL" width="180" height="90" class="center">
            </div>
            </td>
            <td width="75%" valign="middle">
                 <a href="https://www.sciencedirect.com/science/article/abs/pii/S0950705121004159?casa_token=iJLWbW5cPwAAAAAA:oILGjoOQlOhRlKd2YHoYnqVPSDLSUSc-dwyUk0myxoEBn3T7-adXs4R1RB49KmWtB2d3lMxud0M" class="custom-link"> <papertitle>See, Hear, Read: Leveraging Multimodality with Guided Attention for Abstractive Text Summarization</papertitle></a>           
            <br>
            <a href="https://yashkumaratri.github.io/" class="custom-link">Yash Atri*</a>, <b>Shraman Pramanick*</b>, <a href="https://faculty.iiitd.ac.in/~vikram/" class="custom-link">Vikram Goyal</a>, <a href="https://tanmoychak.com/" class="custom-link">Tanmoy Chakraborty</a> 
            <br>
            <em><span style="color: brown;">Knowledge-Based Systems, Elsevier, 2021</span></em>
            <br>
            [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0950705121004159?casa_token=iJLWbW5cPwAAAAAA:oILGjoOQlOhRlKd2YHoYnqVPSDLSUSc-dwyUk0myxoEBn3T7-adXs4R1RB49KmWtB2d3lMxud0M">Paper</a>] [<a href="https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2FLCS2-IIITD%2Fmultimodal_summ&sa=D&sntz=1&usg=AOvVaw1wtGbFeAnzjsDSliXBjCED">Code + Data</a>] </p>
            </td>
          </tr>
          <td><br></td>


          <tr>
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center;">
              <img src="images/ICWSM_System.PNG" alt="ICWSM_MHA_Meme_Model" width="180" height="90" class="center">
            </div>
            </td>
            <td width="75%" valign="middle">
                 <a href="https://ojs.aaai.org/index.php/ICWSM/article/view/18080/17883" class="custom-link"> <papertitle>Exercise? I thought you said ’Extra Fries’: Leveraging Sentence Demarcations and Multi-hop Attention for Meme Affect Analysis</papertitle></a>           
            <br>
            <b>Shraman Pramanick</b>, <a href="https://www.iiitd.ac.in/shad" class="custom-link">Shad Aktar</a>, <a href="https://tanmoychak.com/" class="custom-link">Tanmoy Chakraborty</a> 
            <br> 
            <em><span style="color: brown;">ICWSM 2021</span></em>
            <br>
            [<a href="https://ojs.aaai.org/index.php/ICWSM/article/view/18080/17883">Paper</a>] [<a href="https://github.com/LCS2-IIITD/MHA-MEME">Code</a>] [<a href="https://www.cis.jhu.edu/~shraman/icwsm_presentation.pdf">Slides</a>] [<a href="https://www.cis.jhu.edu/~shraman/ICWSM_Research_Highlights.pdf">Poster</a>]
            </td>
          </tr>
          <td><br></td>


        </tbody></table>

        
        <br>
        <table width="100%" align="center" border="0" cellpadding="0">
          <tr style="padding:0px">
            <td width="100%" valign="middle">
              <a id="news"><heading>Teaching</heading></a>
            </td>
          </tr>
          <tr>
            <td width="75%" valign="center">
              <p>
              <p>I have been worked as TA for the following courses:</p>
              <ul>
                <li>
                  <papertitle>Spring 2023</papertitle>: Machine Intelligence (EN.520.650), Johns Hopkins University
                </li>
                <li>
                  <papertitle>Spring 2022</papertitle>: Machine Intelligence (EN.520.650), Johns Hopkins University
                </li>
              </ul>
              </p>
            </td>
          </tr>
        </table>
          

        <br>
        <table width="100%" align="center" border="0" cellpadding="0">
          <tr style="padding:0px">
            <td width="100%" valign="middle">
              <a id="news"><heading>Selected Honors & Awards</heading></a>
            </td>
          </tr>
          <tr>
            <td width="75%" valign="center">
              <p>
              <ul>
                <li>
                [June, 2024] <a href="https://arxiv.org/pdf/2307.05463.pdf">EgoVLPv2</a> is awarded as an <a href="https://egovis.github.io/awards/2022_2023/">EgoVis (Egocentric Vision) 2022/2023 Distinguished Paper</a>.
		<li> 
		[June, 2024] Received <a href="https://drive.google.com/file/d/1pI-XO8W0Qlnbg-QT7aDsn6gqiFx7dlgH/view?usp=sharing">Spot Bonus</a> from Google for exceptional contributions while being a student researcher.
		</li>
		<li>
                [January, 2021] <a href="https://engineering.jhu.edu/education/graduate-studies/full-time-graduate-fellowships-grants/">JHU ECE Departmental Fellowship</a>, <em>awarded to outstanding incoming Ph.D. students</em>.
                </li>
                <li>
                [May, 2019] <a href="https://www.mitacs.ca/en/programs/globalink/globalink-research-internship">Mitacs Globalink Research Internship</a>, awarded to top-ranked applicants from 15 different countries to participate in a 12-week research internship in Canadian universities.
                </li>
                <li>
                [October, 2016] <a href="https://jbnsts.ac.in/seshprg.php">JBNSTS Senior Scholarship</a>, 4-year scholarship for academic excellence during B.E.
                </li>
              </ul>
              </p>
            </td>
          </tr>
        </table>

        
        <br>
        <table width="100%" align="center" border="0" cellpadding="0">
          <tr style="padding:0px">
            <td width="100%" valign="middle">
              <a id="news"><heading>Voluntary Services</heading></a>
            </td>
          </tr>
          <tr>
            <td width="75%" valign="center">
              <p>
              <p>I have reviewed for:</p>
              <ul>
                <li>
                  <papertitle>Conferences</papertitle>: CVPR, ICCV, ECCV, WACV, ARR, EMNLP, ACL
                </li>
                <li>
                  <papertitle>Jounrals</papertitle>: TPAMI, TNNLS, TIP, TAI, TAFFC, TMLR
                </li>
              </ul>
              </p>
            </td>
          </tr>
        </table>


				

      
					
				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <br>
                Sourced from <a href="https://jonbarron.info/">Jon Barron</a>. Big thanks! 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
